#!/usr/bin/env python3
"""
Script de test pour SkyReels-V2 sur un seul GPU
Pour diagnostiquer les probl√®mes avant d'utiliser multi-GPU
"""

import os
import torch
import gc
import imageio
import time
from diffusers.utils import load_image
from skyreels_v2_infer.modules import download_model
from skyreels_v2_infer.pipelines import Text2VideoPipeline

# Configuration m√©moire optimis√©e
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Utiliser seulement GPU 0

def cleanup_memory():
    """Nettoie la m√©moire GPU"""
    gc.collect()
    torch.cuda.empty_cache()
    if torch.cuda.is_available():
        torch.cuda.synchronize()

def main():
    print("=" * 60)
    print("Test SkyReels-V2 sur GPU unique")
    print("=" * 60)
    
    # Configuration
    model_path = "/home/server/dev/SkyReels-V2-fork/checkpoints/14B-T2V-540P"
    prompt = "A beautiful sunset over the ocean with gentle waves"
    
    # Param√®tres ultra-safe pour test
    num_frames = 17  # Tr√®s peu de frames
    inference_steps = 10  # Peu d'√©tapes
    height = 544
    width = 960
    
    print(f"\nConfiguration de test:")
    print(f"  - Model: {model_path}")
    print(f"  - Frames: {num_frames}")
    print(f"  - Steps: {inference_steps}")
    print(f"  - Resolution: {width}x{height}")
    print(f"  - Device: cuda:0")
    
    # V√©rifier la disponibilit√© GPU
    if not torch.cuda.is_available():
        print("‚ùå CUDA n'est pas disponible!")
        return
    
    print(f"\n‚úÖ GPU d√©tect√©: {torch.cuda.get_device_name(0)}")
    print(f"   M√©moire totale: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # Nettoyer la m√©moire avant de commencer
    cleanup_memory()
    
    try:
        # Initialiser le pipeline (sans USP)
        print("\nüì¶ Chargement du mod√®le...")
        pipe = Text2VideoPipeline(
            model_path=model_path,
            dit_path=model_path,
            use_usp=False,  # Pas de parall√©lisation
            offload=True,   # Offload CPU pour √©conomiser la m√©moire
            weight_dtype=torch.float16,  # float16 pour √©conomiser la m√©moire
            device="cuda:0"
        )
        print("‚úÖ Mod√®le charg√© avec succ√®s")
        
        # Activer TeaCache pour √©conomiser la m√©moire
        print("\nüîß Activation de TeaCache...")
        pipe.transformer.initialize_teacache(
            enable_teacache=True,
            num_steps=inference_steps,
            teacache_thresh=0.35,  # Seuil √©lev√© pour plus d'√©conomie
            use_ret_steps=False,
            ckpt_dir=model_path
        )
        
        # Pr√©parer les arguments
        kwargs = {
            "prompt": prompt,
            "negative_prompt": "blurry, low quality, distorted",
            "num_frames": num_frames,
            "num_inference_steps": inference_steps,
            "guidance_scale": 6.0,
            "shift": 8.0,
            "generator": torch.Generator(device="cuda:0").manual_seed(42),
            "height": height,
            "width": width,
        }
        
        print(f"\nüé¨ G√©n√©ration de la vid√©o...")
        print(f"   Prompt: {prompt}")
        
        # Nettoyer la m√©moire avant g√©n√©ration
        cleanup_memory()
        
        # G√©n√©rer la vid√©o
        start_time = time.time()
        with torch.amp.autocast('cuda', dtype=torch.float16), torch.no_grad():
            video_frames = pipe(**kwargs)[0]
        
        generation_time = time.time() - start_time
        print(f"\n‚úÖ G√©n√©ration r√©ussie en {generation_time:.1f} secondes!")
        
        # Sauvegarder la vid√©o
        os.makedirs("result/test", exist_ok=True)
        output_path = "result/test/test_single_gpu.mp4"
        imageio.mimwrite(output_path, video_frames, fps=24, quality=8, output_params=["-loglevel", "error"])
        print(f"üìπ Vid√©o sauvegard√©e: {output_path}")
        
        # Afficher l'utilisation m√©moire finale
        mem_allocated = torch.cuda.memory_allocated(0) / 1024**3
        mem_reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"\nüìä Utilisation m√©moire GPU:")
        print(f"   Allou√©e: {mem_allocated:.2f} GB")
        print(f"   R√©serv√©e: {mem_reserved:.2f} GB")
        
        print("\n‚úÖ Test termin√© avec succ√®s!")
        
    except torch.cuda.OutOfMemoryError as e:
        print(f"\n‚ùå Erreur de m√©moire GPU!")
        print("   Le mod√®le 14B est trop grand m√™me avec ces param√®tres minimaux.")
        print("\nüí° Solutions possibles:")
        print("   1. Utiliser le mod√®le 1.3B au lieu du 14B")
        print("   2. R√©duire encore num_frames (essayez 9)")
        print("   3. Utiliser une r√©solution plus basse")
        raise e
    
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        raise e
    
    finally:
        # Nettoyer la m√©moire
        cleanup_memory()

if __name__ == "__main__":
    main()